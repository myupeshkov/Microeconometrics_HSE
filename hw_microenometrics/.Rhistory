# Оценим полную модель как комбинацию трех моделей
model.probit.F1.res <- glm(sub~series+internet+TV+age+I(age^2)+I(internet*TV),
data = df[df$residence == "City", ],
family = binomial(link = "probit"))
model.probit.F2.res <- glm(sub~series+internet+TV+age+I(age^2)+I(internet*TV),
data = df[df$residence == "Capital", ],
family = binomial(link = "probit"))
model.probit.F3.res <- glm(sub~series+internet+TV+age+I(age^2)+I(internet*TV),
data = df[df$residence == "Village", ],
family = binomial(link = "probit"))
# Считаем логарифмы правдоподобия
# полной и ограниченной моделей
lnL.F <- logLik(model.probit.F1.res) + logLik(model.probit.F2.res)+logLik(model.probit.F3.res) # логарифм правдоподобия
# полной модели
lnL.R <- logLik(model.probit.R.res)                            # логарифм правдоподобия
# Тестируем гипотезу
r <- length(model.probit.R.res$coefficients) - 2               # число ограничений
t <- 2 * (lnL.F - lnL.R)                                   # статистика теста
p.value <- as.numeric(1 - pchisq(t, df = r))               # p-value теста
p.value
model.logit <- glm(sub~series+internet+TV+age+I(age^2)+I(internet*TV),
data = df,
family=binomial(link = "logit"))
stargazer(model.logit)
# Достанем оценки коэффициентов
coef.logit <- coef(model.logit)
# Отношение шансов - (вероятность успеха) / (вероятность неудачи)
#                    P(y = 1) / P(y = 0)
step <- 1                                                 # приращение для переменных
# Оценим, во сколько раз, при прочих равных,
# изменится отношение шансов при
OR.series <- exp(coef.logit["series"] * step)
step <- 1
OR.age <- exp(coef.logit["age"] * step +                    # изменении возраста на step
coef.logit["I(age^2)"] * step ^ 2 +
2 * coef.logit["I(age^2)"] *
Maks$age * step)
OR.age
step <- 0.1
OR.internet <- exp(coef.logit["internet"] * step + coef.logit["I(internet * TV)"] *
Maks$TV * step)
OR.internet
step <- 1
OR.TV <- exp(coef.logit["TV"] * step + coef.logit["I(internet * TV)"] *
Maks$internet * step)
OR.TV
default_formula <- sub ~ series + internet              # для каждого из уравнений системы
stable_formula <- TV ~ age + internet
model_bp <- gjrm(formula = list(default_formula,        # задаем лист, состоящий из
stable_formula),        # обеих формул
data = df,
Model = "B",                           # указываем тип модели как систему
# бинанрных уравнений
margins = c("probit", "probit"),       # задаем маржинальные распределения
# случайных ошибок уравнений
BivD = "N")                            # задаем тип совместного распределения
# бинарных уравнений
library("pbivnorm")
library("GJRM")                                         # оценивание систем
# бинарных уравнений
library("pbivnorm")
install.packages(GJRM)
install.packages("GJRM")
library("GJRM")                                         # оценивание систем
# бинарных уравнений
library("pbivnorm")
default_formula <- sub ~ series + internet              # для каждого из уравнений системы
stable_formula <- TV ~ age + internet
model_bp <- gjrm(formula = list(default_formula,        # задаем лист, состоящий из
stable_formula),        # обеих формул
data = df,
Model = "B",                           # указываем тип модели как систему
# бинанрных уравнений
margins = c("probit", "probit"),       # задаем маржинальные распределения
# случайных ошибок уравнений
BivD = "N")                            # задаем тип совместного распределения
# случайных ошибок (копулу)
summary(model_bp)                                       # посмотрим на результат
default_formula <- sub ~ series + internet+income              # для каждого из уравнений системы
stable_formula <- TV ~ age + internet
model_bp <- gjrm(formula = list(default_formula,        # задаем лист, состоящий из
stable_formula),        # обеих формул
data = df,
Model = "B",                           # указываем тип модели как систему
# бинанрных уравнений
margins = c("probit", "probit"),       # задаем маржинальные распределения
# случайных ошибок уравнений
BivD = "N")                            # задаем тип совместного распределения
# случайных ошибок (копулу)
summary(model_bp)                                       # посмотрим на результат
rho_est <- model_bp$theta                               # оценка корреляции между случайными ошибками
data.frame("Rho real" = rho,                            # сравним истинное значение корреляции
"Rho estimate" = rho_est)                    # с её оценкой
rho_est
# Оценим полную модель как комбинацию двух моделей
model.probit.F11 <- glm(sub ~ series + internet+income,
data = df,
family = binomial(link = "probit"))
model.probit.F01 <- glm(TV ~ age + internet,
data = df,
family = binomial(link = "probit"))
# Считаем логарифмы правдоподобия
# полной и ограниченной моделей
lnL.F <- logLik(model.probit.F11) + logLik(model.probit.F01) # логарифм правдоподобия
# полной модели
lnL.R <- logLik(model_bp)                            # логарифм правдоподобия
# Тестируем гипотезу
r <- length(model.probit.R.male$coefficients) - 2               # число ограничений
t <- 2 * (lnL.F - lnL.R)                                   # статистика теста
p.value <- as.numeric(1 - pchisq(t, df = r))               # p-value теста
p.value
# Тестируем гипотезу
r <- length(model_bp$coefficients) - 2               # число ограничений
t <- 2 * (lnL.F - lnL.R)                                   # статистика теста
p.value <- as.numeric(1 - pchisq(t, df = r))               # p-value теста
p.value
View(model_bp)
logLik(model_bp)
p.value <- as.numeric(1 - pchisq(t, df = 1))               # p-value теста
p.value
logLik(model.probit.F11) + logLik(model.probit.F01)
rho_est
model_bp <- gjrm(formula = list(default_formula,        # задаем лист, состоящий из
stable_formula),        # обеих формул
data = h,
Model = "B",                           # указываем тип модели как систему
# бинанрных уравнений
margins = c("probit", "probit"),       # задаем маржинальные распределения
# случайных ошибок уравнений
BivD = "N")                            # задаем тип совместного распределения
# случайных ошибок (копулу)
summary(model_bp)                                       # посмотрим на результат
cov_est <- solve(model_bp$fit$hessian)                  # оценка асимптотической ковариационной матрицы
std_rho <- sqrt(cov_est["theta.star", "theta.star"])    # оценка стандартной ошибки оценки корреляции
p_value_rho <- 2 * min(pnorm(rho_est / std_rho),        # p-value теста о равенстве корреляции между
1- pnorm(rho_est / std_rho))
p_value_rho
cov_est
p_value_rho
histogram(log(df[,'income'])) #доход как всегда лог-нормальный
histogram(df[,'income'])
coefficients(model_bp, eq = 1)
individ <- data.frame(series = 4,                             # мои характеристики
internet = 0.7,
income=50000,
age = 21)
coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income
coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series+
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income
pnorm(coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series+
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income)
pnorm(coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet)
coefficients(model_bp, eq = 1)
coefficients(model_bp, eq = 2)
pnorm(coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series+
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income+
coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet)
model_bp <- gjrm(formula = list(default_formula,        # задаем лист, состоящий из
stable_formula),        # обеих формул
data = df,
Model = "B",                           # указываем тип модели как систему
# бинанрных уравнений
margins = c("probit", "probit"),       # задаем маржинальные распределения
# случайных ошибок уравнений
BivD = "N",
)
p_1_2 <- pbivnorm(x = cbind(default_li,                 # аргументы функции распределения
stable_li),                 # двумерного нормального распределения,
# расположенные по строкам
rho = rho_est)
default_li <- predict(model_bp, eq = 1)
stable_li <- predict(model_bp, eq = 2)
p_1_2 <- pbivnorm(x = cbind(default_li,                 # аргументы функции распределения
stable_li),                 # двумерного нормального распределения,
# расположенные по строкам
rho = rho_est)
p_1_2
pnorm(coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series+
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income)
pnorm(coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet)
default_li <- predict(model_bp, eq = 1)
stable_li <- predict(model_bp, eq = 2)
p_1_2 <- pbivnorm(x = cbind(coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet,
coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet),
rho = rho_est)
p_1_2
p_1_n2 <- pbivnorm(x = cbind(coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet,
-(coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet)),
rho = -rho_est)
p_1_cond_2 <- p_1_n2 / (1- p_2)
p_1 <-pnorm(coefficients(model_bp, eq = 1)[1]+coefficients(model_bp, eq = 1)[2]*individ$series+
+coefficients(model_bp, eq = 1)[3]*individ$internet+ coefficients(model_bp, eq = 1)[4]*individ$income)
p_2 <-pnorm(coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet)
default_li <- predict(model_bp, eq = 1)
stable_li <- predict(model_bp, eq = 2)
p_1_2 <- pbivnorm(x = cbind(coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet,
coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet),
rho = rho_est)
p_1_2
p_1_n2 <- pbivnorm(x = cbind(coefficients(model_bp, eq = 1)[5]+coefficients(model_bp, eq = 1)[6]*individ$age+
+coefficients(model_bp, eq = 1)[7]*individ$internet,
-(coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet)),
rho = -rho_est)
p_1_cond_2 <- p_1_n2 / (1- )
p_1_cond_2 <- p_1_n2 / (1- p2)
p_2 <-pnorm(coefficients(model_bp, eq = 2)[5]+coefficients(model_bp, eq = 2)[6]*individ$age+
+coefficients(model_bp, eq = 2)[7]*individ$internet)
p_1_cond_2 <- p_1_n2 / (1- p2)
p_1_cond_2 <- p_1_n2 / (1- p_2)
p_1_cond_2
# логит модель
probs.logit <- predict(model.logit,  type = "response")    # вероятность
def.logit <- as.numeric(probs.logit >= 0.5)          # дефолт
correct.logit <- mean(def.logit == h$def)
correct.logit <- mean(def.logit == df$sub)
# логит модель
probs.logit <- predict(model.logit,  type = "response")    # вероятность
sub.logit <- as.numeric(probs.logit >= 0.5)          # дефолт
correct.logit <- mean(sub.logit == df$sub)
#система
probs.logit <- predict(model_bp,  type = "response")    # вероятность
#система
probs.logit <- predict(model_bp(eq=1),  type = "response")    # вероятность
#система
probs.logit <- predict(model_bp, eq = 1)    # вероятность
sub.sys <- as.numeric(probs.sys >= 0.5)          # дефолт
#система
probs.sys <- predict(model_bp, eq = 1)    # вероятность
sub.sys <- as.numeric(probs.sys >= 0.5)          # дефолт
correct.sys <- mean(sub.sys == df$sub)
rbind(probit = correct.probit,                             # пробит модель
naive = correct.naive,# наивная модель
linear = correct.linear,#линейная
logit= correct.logit,
sys = correct.sys,
)
rbind(probit = correct.probit,                             # пробит модель
naive = correct.naive,# наивная модель
linear = correct.linear,#линейная
logit= correct.logit,
sys = correct.sys
)
rbind(probit = AIC(model.probit),                             # пробит модель
linear = AIC(model.linear),#линейная
logit= AIC(model.logit),
sys = AIC(model.bp, eq=1)
)
rbind(probit = AIC(model.probit),                             # пробит модель
linear = AIC(model.linear),#линейная
logit= AIC(model.logit),
sys = AIC(model_bp, eq=1)
)
rbind(probit = AIC(model.probit),                             # пробит модель
linear = AIC(model.linear),#линейная
logit= AIC(model.logit),
sys = AIC(model_bp)
)
rbind(probit = BIC(model.probit),                             # пробит модель
linear = BIC(model.linear),#линейная
logit= BIC(model.logit),
sys = BIC(model_bp)
)
model.linear1 <- lm(default_formula, data=df)
model.linear1 <- lm(default_formula, data=df)
model.linear2 <- lm(stable_formula, data=df)
aic.linear <- AIC(model.linear1)+AIC(model.linear2)
model.probi1 <- glm(default_formula, data=df, family=binomial(link = "probit"))
model.probit2 <- glm(stable_formula, data=df,family=binomial(link = "probit"))
aic.probit <- AIC(model.probi1)+AIC(model.probi2)
model.probit1 <- glm(default_formula, data=df, family=binomial(link = "probit"))
model.probit2 <- glm(stable_formula, data=df,family=binomial(link = "probit"))
aic.probit <- AIC(model.probit1)+AIC(model.probit2)
model.linear1 <- lm(default_formula, data=df)
model.linear2 <- lm(stable_formula, data=df)
aic.linear <- AIC(model.linear1)+AIC(model.linear2)
bic.linear <- BIC(model.linear1)+BIC(model.linear2)
model.probit1 <- glm(default_formula, data=df, family=binomial(link = "probit"))
model.probit2 <- glm(stable_formula, data=df,family=binomial(link = "probit"))
aic.probit <- AIC(model.probit1)+AIC(model.probit2)
bic.probit <- BIC(model.probit1)+BIC(model.probit2)
model.logit1 <- glm(default_formula, data=df, family=binomial(link = "logit"))
model.logit2 <- glm(stable_formula, data=df,family=binomial(link = "logit"))
aic.logit <- AIC(model.logit1)+AIC(model.logit2)
bic.logit <- BIC(model.logit1)+BIC(model.logit2)
rbind(probit = aic.probit,                             # пробит модель
linear = aic.linear,#линейная
logit= aic.logit,
sys = AIC(model_bp)
)
rbind(probit = bic.probit,                             # пробит модель
linear = bic.linear,#линейная
logit= bic.logit,
sys = BIC(model_bp)
)
ME.series.het.sd
set.seed(123)
eps <- rt(n, 13)
beta <- c(2, 1.5, -0.7, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
n <- 5000                                           # число индивидов в выборке
h <- data.frame(income = exp(rnorm(n, 10, 0.7)))     # доход
h$age = round(runif(n, 20, 100))                     # возраст
h$сigarette <- rbinom(n, 1, 0.4)                      # дамми на курильщиков
eps <- rt(n, 13)
beta <- c(2, 1.5, -0.7, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$def)
mean(h$coffee)
beta <- c(8, 0.5, -0.7, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
beta <- c(1, 0.5, -0.7, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
beta <- c(1, 0.5, -0.07, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
beta <- c(1, 0.5, -0.12, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
beta <- c(1, 0.5, -0.07, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
beta <- c(1, 0.5, -0.09, 0.2)
cofe_li <- beta[1] +                                        # линейный индекс,
beta[2] * log(h$inc) +                                # отражающий вклад наблюдаемых факторов в вероятность частого употребления кофе
beta[3] * h$age +
beta[4] * h$сigarette
cofe_star <- cofe_li + eps                                   # латентная переменная,
# отражающая склонность
# к дефолту
h$coffee <- as.numeric(cofe_star >= 0)                         # наблюдаемое значение переменной
mean(h$coffee)
# Итоговые данные
head(h, 10)
# Будем оценивать регрессию не
# предполагая заранее наличие константы
modelLnL <- function(beta, y, X)                          # функция правдоподобия
{
beta <- matrix(beta, ncol = 1)                           # вектор оптимизируемых параметров, то есть
# регрессионных коэффициентов,
# переводим в матрицу с одним столбцом
y_est <- X %*% beta                                      # оценка математического ожидания
# латентной переменной
n_obs <- nrow(X)                                         # количество наблюдений
L_vec <- matrix(NA, nrow = n_obs,                        # вектор столбец вкладов наблюдений
ncol = 1)                                # в функцию правдоподобия
is_y_0 <- y == 0                                         # вектор условий y = 0
is_y_1 <- y == 1                                         # вектор условий y = 1
L_vec[is_y_1] <- pt(y_est[is_y_1], 13)                    # вклад наблюдений для которых yi = 1
L_vec[is_y_0] <- 1 - pt(y_est[is_y_0], 13)                # вклад наблюдений для которых yi = 0
lnL <- sum(log(L_vec))                                   # логарифм функции правдоподобия
return(lnL)
}
# Пробит регрессия
student_model <- function(formula,                                # формула
data)                                   # датафрейм
{
d <- model.frame(formula, data)                          # извлекаем переменные согласно формуле
y <- as.matrix(d[, 1], ncol = 1)                         # зависимая переменная как первый
# столбец в d
X <- as.matrix(d[, -1])                                  # независимые переменные как все переменные
# из data кроме зависимой
X <- cbind(1, X)                                         # добавим константу
x0_n <- ncol(X)                                          # число оцениваемых параметров
X_names <- names(d)[-1]                                  # имена независимых переменных
X_names <- c("Intercept", X_names)                       # добавляем имя константе
result <- optim(par = rep(0, x0_n),                      # в качестве начальных точек возьмем нули
method = "BFGS",                         # численный метод оптимизации
fn = modelLnL,                          # максимизируемая функция правдоподобия
control = list(maxit = 10000,            # чтобы минимизационную задачу превратить
fnscale = -1,             # в максимизационную умножаем функцию на -1
reltol = 1e-10),          # установим достаточно высокую точность
hessian = TRUE,                          # вернем Гессиан функции
X = X, y = y)                            # аргументы оптимизируемой функции
beta.est <- result$par                                   # оценки коэффициентов
names(beta.est) <- X_names                               # сопоставляем имена для оценок коэффициентов
as.cov.est <- solve(-result$hessian)                     # оценка асимптотической ковариационной
colnames(as.cov.est) <- X_names                          # матрицы полученных оценок
rownames(as.cov.est) <- X_names                          # сопоставляем имена
return_list <- list("beta" = beta.est,                   # возвращаем оценки коэффициентов и
"cov" = as.cov.est,                  # асимптотической ковариационной матрицы
"data" = data,                       # возвращаем использовавшийся датафрейм
"lnL" = result$value,                # возвращаем логарифм функции правдоподобия
"X" = X,                             # возвращаем матрицу регрессоров
"y" = y,                             # возвращаем зависимую переменную
"formula" = formula)                 # возвращаем использовавшуюся формулу
class(return_list) <- "probit"                           # для удобства назначим класс
# возвращаемой из функции переменной
return(return_list)                                      # возвращаем результат
}
#регрессия
student_model <- function(formula,                                # формула
data)                                   # датафрейм
{
d <- model.frame(formula, data)                          # извлекаем переменные согласно формуле
y <- as.matrix(d[, 1], ncol = 1)                         # зависимая переменная как первый
# столбец в d
X <- as.matrix(d[, -1])                                  # независимые переменные как все переменные
# из data кроме зависимой
X <- cbind(1, X)                                         # добавим константу
x0_n <- ncol(X)                                          # число оцениваемых параметров
X_names <- names(d)[-1]                                  # имена независимых переменных
X_names <- c("Intercept", X_names)                       # добавляем имя константе
result <- optim(par = rep(0, x0_n),                      # в качестве начальных точек возьмем нули
method = "BFGS",                         # численный метод оптимизации
fn = modelLnL,                          # максимизируемая функция правдоподобия
control = list(maxit = 10000,            # чтобы минимизационную задачу превратить
fnscale = -1,             # в максимизационную умножаем функцию на -1
reltol = 1e-10),          # установим достаточно высокую точность
hessian = TRUE,                          # вернем Гессиан функции
X = X, y = y)                            # аргументы оптимизируемой функции
beta.est <- result$par                                   # оценки коэффициентов
names(beta.est) <- X_names                               # сопоставляем имена для оценок коэффициентов
as.cov.est <- solve(-result$hessian)                     # оценка асимптотической ковариационной
colnames(as.cov.est) <- X_names                          # матрицы полученных оценок
rownames(as.cov.est) <- X_names                          # сопоставляем имена
return_list <- list("beta" = beta.est,                   # возвращаем оценки коэффициентов и
"cov" = as.cov.est,                  # асимптотической ковариационной матрицы
"data" = data,                       # возвращаем использовавшийся датафрейм
"lnL" = result$value,                # возвращаем логарифм функции правдоподобия
"X" = X,                             # возвращаем матрицу регрессоров
"y" = y,                             # возвращаем зависимую переменную
"formula" = formula)                 # возвращаем использовавшуюся формулу
return(return_list)                                      # возвращаем результат
}
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age + cigarette
data = h)
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age + cigarette,
data = h)
# Итоговые данные
head(h, 10)
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age + cigarette,
data = h)
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age,
data = h)
model$beta                                     # получаем оценки коэффициентов
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age+cigarette,
data = h)
# Воспользуемся созданной функцией
model <- student_model(coffee ~ log(income) + age+сigarette,
data = h)
model$beta                                     # получаем оценки коэффициентов
beta.est <- model$beta                                     # получаем оценки коэффициентов
beta.cov.est <- model$cov                                  # получаем оценку асимптотической
as_std_est <- sqrt(diag(beta.cov.est))                      # вычисляем оценки асимптотических
z <- beta.est / as_std_est                               # считаем тестовые статистики
p_values <- 2 * pmin(pnorm(z), 1 - pnorm(z))
p_values
z
